{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPHebrKWd5E6O+TC2agxPuq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AugustvonMackensen/AI_colab/blob/main/2_3_LR_Variable.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2_3_LR_Variable.ipynb**"
      ],
      "metadata": {
        "id": "rAxyLsxRKWXg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variable & Automatic Gradient Calculation**\n",
        "- Tensor vs Variable\n",
        "- Graph and Gradient\n"
      ],
      "metadata": {
        "id": "XE7bNfnNKc1H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch # Facebook에서 개발함\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "Whru8VF3KkoY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Tensor & Variable**\n",
        "\n",
        "**1) Declaration**"
      ],
      "metadata": {
        "id": "KLR4y7nBLkGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_tensor = torch.Tensor(3,4)\n",
        "x_tensor"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "em_ZgJu_LtYX",
        "outputId": "c94efbde-db97-446b-c34a-5eaf6a330452"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.5101e-35, 0.0000e+00, 3.7835e-44, 0.0000e+00],\n",
              "        [       nan, 0.0000e+00, 1.3733e-14, 6.4069e+02],\n",
              "        [4.3066e+21, 1.1824e+22, 4.3066e+21, 6.3828e+28]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_variable = Variable(x_tensor)\n",
        "x_variable"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-gxDPDHL_fu",
        "outputId": "58fe8386-8d00-4cd2-f90e-c5fd9d3c7208"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.5101e-35, 0.0000e+00, 3.7835e-44, 0.0000e+00],\n",
              "        [       nan, 0.0000e+00, 1.3733e-14, 6.4069e+02],\n",
              "        [4.3066e+21, 1.1824e+22, 4.3066e+21, 6.3828e+28]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Variable은 autogradient 가능함<br>\n",
        "Pytorch 0.4 이상 버전에서는 Tensor에 Variable이 통합되고,<br>\n",
        "Variable은 deprecated 됨<br>\n",
        "Tensor만 사용함<br>\n",
        "\n",
        "**2) Variables of a Variable**\n"
      ],
      "metadata": {
        "id": "Tg2USIslMOIf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data 속성\n",
        "x_variable.data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S1cvD3hEMnAe",
        "outputId": "096252ed-0d23-418c-bd9d-b192cfa937d9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1.5101e-35, 0.0000e+00, 3.7835e-44, 0.0000e+00],\n",
              "        [       nan, 0.0000e+00, 1.3733e-14, 6.4069e+02],\n",
              "        [4.3066e+21, 1.1824e+22, 4.3066e+21, 6.3828e+28]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# grad 속성 : 값에 대한 gradient\n",
        "# Variable 생성시 초기화되면서, gradient도 같이 정의됨\n",
        "print(x_variable.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IfDbreJpM103",
        "outputId": "42c6c4aa-98da-41bd-ca86-b2764ffa6274"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# requires_grad 속성 : 값에 대한 gradient 요구시 사용\n",
        "print(x_variable.requires_grad) # 속성값 확인 : False\n",
        "# gradient 가 계산이 안된 상태임"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9dYKmwjiNVZG",
        "outputId": "be23b2e1-ea11-4dad-e28e-8526fa675a61"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# gradient 에 대한 연산을 모두 추적함 : True\n",
        "x_variable = Variable(x_tensor, requires_grad=True)\n",
        "print(x_variable.requires_grad)\n",
        "print(x_variable.data)\n",
        "print(x_variable.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gzx0KvDmOH_f",
        "outputId": "ce922f20-4b30-4cdd-9674-4282944a1cfa"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "tensor([[1.5101e-35, 0.0000e+00, 3.7835e-44, 0.0000e+00],\n",
            "        [       nan, 0.0000e+00, 1.3733e-14, 6.4069e+02],\n",
            "        [4.3066e+21, 1.1824e+22, 4.3066e+21, 6.3828e+28]])\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# volatile 속성 : 최소 메모리 사용에 대한 설정\n",
        "# requires_grad가 False이면, volatile 도 자동 False가 됨\n",
        "x_variable = Variable(x_tensor, volatile=True)\n",
        "print(x_variable.grad)\n",
        "print(x_variable.volatile)\n",
        "print(x_variable.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zfBzwmDmOtvv",
        "outputId": "ae3b01ab-53fa-4418-a10f-81442ce38b81"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "None\n",
            "False\n",
            "False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:5: UserWarning: volatile was removed (Variable.volatile is always False)\n",
            "  \"\"\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **3. Graph & Variables**"
      ],
      "metadata": {
        "id": "pXjMUYeHQRZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create graph\n",
        "x = Variable(torch.FloatTensor(3, 4), requires_grad=True)\n",
        "y = x ** 2 + 4 * x\n",
        "z = 2 * y + 3\n",
        "\n",
        "print(x.requires_grad)\n",
        "print(y.requires_grad)\n",
        "print(z.requires_grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "smPK1JEfQVlH",
        "outputId": "a19df262-3938-453d-cc47-8bb2dbe1033b"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "True\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# backward(gradient, retain_graph, create_graph, retain_variables)\n",
        "# 현재 값 w, r, t, graph에 대한 gradient 계산 함수임\n",
        "# 역전파 알고리즘 적용 계산함수\n",
        "\n",
        "# 위의 z 값으로 x의 gradient 를 계산해 냄\n",
        "loss = torch.FloatTensor(3, 4)\n",
        "z.backward(loss)\n",
        "\n",
        "print(x.grad)\n",
        "print(y.grad)\n",
        "print(z.grad)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G7Hy390mV1HX",
        "outputId": "300c3395-7e36-46c7-8958-e912268b3c57"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[8.7099e-37, 0.0000e+00, 9.4167e-42, 9.8651e-42],\n",
            "        [1.0403e-41, 3.5873e-42, 1.0762e-41, 4.1254e-42],\n",
            "        [9.2374e-42, 1.0224e-41, 8.6993e-42, 8.9683e-42]])\n",
            "None\n",
            "None\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/_tensor.py:1083: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the .grad field to be populated for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations. (Triggered internally at  aten/src/ATen/core/TensorBody.h:477.)\n",
            "  return self._grad\n"
          ]
        }
      ]
    }
  ]
}